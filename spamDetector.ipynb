{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Detector \n",
    "## Charles James\n",
    "\n",
    "### In this project, the model is built to determine whether an email is spam or not. Included below is an implementation of the Naive Bayes model, one of the known types of classification, on the training and testing data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tBuild a model to predict whether an email is a spam or not.\n",
    "\n",
    "In this program, Spam is noted as a 1 and not spam is noted as 0 under the label_num column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#natural laguage tool kit\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>605</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: enron methanol ; meter # : 988291\\r\\n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2349</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: hpl nom for january 9 , 2001\\r\\n( see...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3624</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: neon retreat\\r\\nho ho ho , we ' re ar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4685</td>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: photoshop , windows , office . cheap ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2030</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: re : indian springs\\r\\nthis deal is t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 label                                               text  \\\n",
       "0         605   ham  Subject: enron methanol ; meter # : 988291\\r\\n...   \n",
       "1        2349   ham  Subject: hpl nom for january 9 , 2001\\r\\n( see...   \n",
       "2        3624   ham  Subject: neon retreat\\r\\nho ho ho , we ' re ar...   \n",
       "3        4685  spam  Subject: photoshop , windows , office . cheap ...   \n",
       "4        2030   ham  Subject: re : indian springs\\r\\nthis deal is t...   \n",
       "\n",
       "   label_num  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  \n",
       "3          1  \n",
       "4          0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read the CSV file\n",
    "df = pd.read_csv('spam_ham_dataset.csv')\n",
    "\n",
    "#print the first 5 rows\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5171, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print the shape(number of rows and columns)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'label', 'text', 'label_num'], dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get the column names\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0    0\n",
       "label         0\n",
       "text          0\n",
       "label_num     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Show the number of missing data for each column (Examples: NAN, NaN, na)\n",
    "#Here we check to see if any of the mail in the csv has missing data. \n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/cj/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Download the stopwords package\n",
    "#We download the stopwords package to use in the function that will be created. \n",
    "#Stopwords are the English words which does not add much meaning to a sentence. \n",
    "#They can safely be ignored without sacrificing the meaning of the sentence.\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The goal of this function is to process the texts in 3 ways. \n",
    "\n",
    "#The function process_text takes in the text parameter (email message)\n",
    "def process_text(text):\n",
    "    \n",
    "    #1) The first way is removing punctuation from the text. It is then stored in nopunc\n",
    "    nopunc = [char for char in text if char not in string.punctuation]\n",
    "    nopunc = ''.join(nopunc)\n",
    "    \n",
    "    #2) The second way is removing the stopwords(useless words/data) from the text. \n",
    "    # Then we split the message to get tokens. Each word will be sepereated by a comma.\n",
    "    # We also specifiy that the top words will be english. \n",
    "    clean_words = [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]\n",
    "    \n",
    "    #3) The last way is returning a list of clean text words\n",
    "    return clean_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [Subject, enron, methanol, meter, 988291, foll...\n",
       "1    [Subject, hpl, nom, january, 9, 2001, see, att...\n",
       "2    [Subject, neon, retreat, ho, ho, ho, around, w...\n",
       "3    [Subject, photoshop, windows, office, cheap, m...\n",
       "4    [Subject, indian, springs, deal, book, teco, p...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Show the tokenization ( a list of tokens also called lemmas)\n",
    "#A lemma is a word that stands at the head of a definition in a dictionary\n",
    "#Since 'text' column contains the messages, we apply the function we just created to the 'text' column and print out the first 5 rows.\n",
    "df['text'].head(5).apply(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Subject: enron methanol ; meter # : 988291\\r\\n...\n",
       "1    Subject: hpl nom for january 9 , 2001\\r\\n( see...\n",
       "2    Subject: neon retreat\\r\\nho ho ho , we ' re ar...\n",
       "3    Subject: photoshop , windows , office . cheap ...\n",
       "4    Subject: re : indian springs\\r\\nthis deal is t...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For comparison to show difference from function. \n",
    "df['text'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world hello hello world play\n",
      "test test test test one hello\n",
      "\n",
      "  (0, 0)\t3\n",
      "  (0, 4)\t2\n",
      "  (0, 2)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 3)\t4\n",
      "  (1, 1)\t1\n",
      "\n",
      "(2, 5)\n"
     ]
    }
   ],
   "source": [
    "#This cell shows an example of how we will input the processed text into the model to make predict if the message is spam or not.\n",
    "\n",
    "# We create two strings pretending they are messages. \n",
    "message0 = 'hello world hello hello world play'\n",
    "message1 = 'test test test test one hello'\n",
    "print(message0)\n",
    "print(message1)\n",
    "print()\n",
    "\n",
    "#Convert the text to a matrix of token counts. \n",
    "#We use the CountVectorizer to both tokenize the collection of text and build a vocabulary of known words. \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#bow is short for bag of words. \n",
    "#We use the transform method, specifiying the the two messages we created making it a list of lists.\n",
    "#The function created is used as the analyzer.\n",
    "bow4 = CountVectorizer(analyzer=process_text).fit_transform([[message0], [message1]])\n",
    "\n",
    "print(bow4)\n",
    "print()\n",
    "#Shows the number of messages(rows) and the number of unique words in the data set(columns)\n",
    "print(bow4.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first column of numbers, 0 is for message 0 and 1 is for message one. The last column tells us that a unique word appears that many times. For example, the first row:\n",
    "(0, 0)   3 \n",
    "tells us that a word in message 0 appears 3 times. This word is \"hello\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert a collection of text(messages) to a matrix of tokens\n",
    "#We now use the transform method on the text in out dataset\n",
    "messages_bow = CountVectorizer(analyzer = process_text).fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into 80% training and 20% testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#We split the data on messages_bow and on the 'label_num' column\n",
    "x_train, x_test, y_train, y_test = train_test_split(messages_bow, df['label_num'], test_size = 0.20, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5171, 50381)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get the shape of messages_bow\n",
    "messages_bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create and train the Naive Bayes Model to make our prediction\n",
    "#We use the multinomial Naive Bayes classifier (suitible for discrete features Ex: words counts from a text)\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "#fit method is used to train it\n",
    "classifier = MultinomialNB().fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 1 0 0]\n",
      "[0 0 0 ... 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "#Print the predictions\n",
    "print(classifier.predict(x_train))\n",
    "\n",
    "#print the actual values\n",
    "print(y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      2940\n",
      "           1       0.98      0.97      0.98      1196\n",
      "\n",
      "    accuracy                           0.99      4136\n",
      "   macro avg       0.99      0.98      0.98      4136\n",
      "weighted avg       0.99      0.99      0.99      4136\n",
      "\n",
      "\n",
      "Confusion Matrix: \n",
      " [[2918   22]\n",
      " [  30 1166]]\n",
      "True Positive | False Positive\n",
      "False Negative | True Negative\n",
      "\n",
      "Accuracy:  0.9874274661508704\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the model on the training data set\n",
    "#imports\n",
    "#Classification report is used to measure the quality of predictions from a classification algorithm\n",
    "from sklearn.metrics import classification_report\n",
    "#A confusion matrix is a tabular summary of the number of correct and incorrect predictions made by a classifier. \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "pred = classifier.predict(x_train)\n",
    "print(classification_report(y_train, pred))\n",
    "print()\n",
    "print('Confusion Matrix: \\n', confusion_matrix(y_train, pred))\n",
    "print('True Positive | False Positive')\n",
    "print('False Negative | True Negative')\n",
    "print()\n",
    "print('Accuracy: ', accuracy_score(y_train, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We Now Do the same thing for the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 1 0]\n",
      "[0 0 0 ... 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "#Print the predictions\n",
    "print(classifier.predict(x_test))\n",
    "\n",
    "#print the actual values\n",
    "print(y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       732\n",
      "           1       0.95      0.96      0.96       303\n",
      "\n",
      "    accuracy                           0.97      1035\n",
      "   macro avg       0.97      0.97      0.97      1035\n",
      "weighted avg       0.97      0.97      0.97      1035\n",
      "\n",
      "\n",
      "Confusion Matrix: \n",
      " [[718  14]\n",
      " [ 13 290]]\n",
      "True Positive | False Positive\n",
      "False Negative | True Negative\n",
      "\n",
      "Accuracy:  0.9739130434782609\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the model on the test data set\n",
    "pred = classifier.predict(x_test)\n",
    "print(classification_report(y_test, pred))\n",
    "print()\n",
    "print('Confusion Matrix: \\n', confusion_matrix(y_test, pred))\n",
    "print('True Positive | False Positive')\n",
    "print('False Negative | True Negative')\n",
    "print()\n",
    "print('Accuracy: ', accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
